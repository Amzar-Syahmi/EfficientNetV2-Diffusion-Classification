{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4296,"status":"ok","timestamp":1718479868409,"user":{"displayName":"­암자르 | 데이터사이언스전공 | 한양대(서울)","userId":"11308499139223686393"},"user_tz":-540},"id":"i2CoyXZ7waCS"},"outputs":[],"source":["#imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset\n","from torchvision import transforms, datasets\n","from timm import create_model\n","import timm.data\n","import timm\n","import time\n","import os\n","import pandas as pd\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torch.optim.lr_scheduler import _LRScheduler\n","from sklearn.model_selection import KFold\n","import torch.nn.functional as F\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1718479868437,"user":{"displayName":"­암자르 | 데이터사이언스전공 | 한양대(서울)","userId":"11308499139223686393"},"user_tz":-540},"id":"hxguerJ8uIVt","outputId":"f974692b-7610-42db-99e0-d640dfd1fa07"},"outputs":[],"source":["#checking CUDA-torch compatibility\n","!nvcc --version\n","print(f\"\\ntorch CUDA version: {torch.cuda_version}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1718479868447,"user":{"displayName":"­암자르 | 데이터사이언스전공 | 한양대(서울)","userId":"11308499139223686393"},"user_tz":-540},"id":"b3jEC7VvwpOu","outputId":"c0ae5f05-fe31-4ce4-ea39-94b7a00f314b"},"outputs":[],"source":["#checking for GPU availability (local_runtime)\n","print(f'CUDA available: {torch.cuda.is_available()}')\n","print(f'Number of GPUs: {torch.cuda.device_count()}')\n","print(f'Current GPU: {torch.cuda.current_device()}')\n","print(f'GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}')"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["#creation of custom dataset\n","class DeepfakeDataset(Dataset):\n","    def __init__(self, root_dir, transform=None, multi= False):\n","        self.multi= multi\n","        if self.multi:\n","          self.dataset= ConcatDataset([datasets.ImageFolder(root= root, transform= transform) for root in root_dir])\n","        else:\n","          self.dataset= datasets.ImageFolder(root=root_dir, transform=transform)\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n","      \n","#corruption function\n","def corrupt_data(image, alpha_t):\n","    image_cpu= image.cpu().numpy()\n","    noise= np.random.normal(0, 1, image_cpu.shape)\n","    corrupted_data= np.sqrt(alpha_t)* image_cpu+ np.sqrt(1 - alpha_t)* noise\n","    \n","    return torch.tensor(corrupted_data).to(image.device)\n","\n","#model training sqeuence\n","def train_model_fine(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, type='efficient'):\n","    start_time= time.time()\n","\n","    train_losses= []\n","    val_losses= []\n","    accuracies= []\n","    best_accuracy= 0.0\n","    best_val_loss= 0.0\n","\n","    for epoch in range(num_epochs):\n","        current_time= time.time()\n","        \n","        #training sequence\n","        model.train()\n","        running_loss= 0.0\n","        print(f\"Epoch {epoch+1}/{num_epochs} is Starting\")\n","\n","        for images, labels in train_loader:\n","            images, labels= images.to('cuda'), labels.to('cuda')\n","            optimizer.zero_grad()\n","\n","            if type== 'diffusion':\n","              corrupted_data= images.clone()\n","              for i in range(corrupted_data.size(0)// 2):\n","                  alpha_t= np.random.uniform(0.1, 0.9)\n","                  corrupted_data[i]= corrupt_data(images[i], alpha_t)\n","              \n","              outputs= model(corrupted_data).to('cuda')\n","              loss= criterion(outputs, labels)\n","            \n","            else:\n","              outputs= model(images)\n","              loss= criterion(outputs, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss+= loss.item()\n","\n","        epoch_loss= running_loss/ len(train_loader)\n","        train_losses.append(epoch_loss)\n","        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n","\n","        #validation sequence\n","        model.eval()\n","        val_loss= 0.0\n","        correct= 0\n","        total= 0\n","\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels= images.to('cuda'), labels.to('cuda')\n","\n","                if type== 'diffusion':\n","                  corrupted_data= images.clone()\n","                  for i in range(corrupted_data.size(0)// 2):\n","                      alpha_t= np.random.uniform(0.1, 0.9)\n","                      corrupted_data[i]= corrupt_data(images[i], alpha_t)\n","                      \n","                  outputs= model(corrupted_data).to('cuda')\n","                  loss= criterion(outputs, labels)\n","                \n","                else:\n","                  outputs= model(images)\n","                  loss= criterion(outputs, labels)\n","                \n","                val_loss+= loss.item()\n","                _, predicted= torch.max(outputs.data, 1)\n","                total+= labels.size(0)\n","                correct+= (predicted== labels).sum().item()\n","\n","        val_loss/= len(val_loader)\n","        accuracy= 100 * correct/ total\n","        val_losses.append(val_loss)\n","        accuracies.append(accuracy)\n","\n","        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n","        print(\"--- %s Seconds ---\\n\" % (time.time()- current_time))\n","        \n","        #early stoppage\n","        if best_accuracy> accuracy and best_val_loss< val_loss:\n","          print(f'Early Stopping at Epoch {epoch+ 1}')\n","          break\n","        else:\n","          best_accuracy= accuracy\n","          best_val_loss= val_loss\n","\n","        scheduler.step()\n","\n","    print('Training Complete')\n","    print(\"Total Time:--- %s Minutes ---\" % ((time.time()- start_time)/ 60))\n","\n","    #plotting loss and accuracy\n","    plt.figure(figsize=(10, 5))\n","\n","    #plotting training and validation loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Training Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.legend()\n","\n","    #plotting validation accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(accuracies, label='Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy (%)')\n","    plt.title('Validation Accuracy')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":678,"status":"ok","timestamp":1718479869132,"user":{"displayName":"­암자르 | 데이터사이언스전공 | 한양대(서울)","userId":"11308499139223686393"},"user_tz":-540},"id":"xcpu1-iuyejL","outputId":"70c4a5bd-ced4-4332-a398-319bcd0f1980"},"outputs":[],"source":["#dataset configurations\n","data_cfg= timm.data.resolve_data_config({'input_size': (3, 224, 224)})\n","data_transforms= {\n","    'train': transforms.Compose([timm.data.create_transform(**data_cfg, is_training= True),\n","                                 transforms.Normalize(mean= [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225])]),\n","    'val': transforms.Compose([timm.data.create_transform(**data_cfg, is_training= False),\n","                                 transforms.Normalize(mean= [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225])]),\n","}\n","\n","#dataset paths\n","train_data_path= r'TRAINING_IMAGES_PATHS'\n","val_data_path= r'VALIDATION_IMAGES_PATHS'\n","\n","#initializing datasets\n","train_dataset_fine= DeepfakeDataset(root_dir= train_data_path, transform= data_transforms['train'])\n","val_dataset_fine= DeepfakeDataset(root_dir= val_data_path, transform= data_transforms['val'], multi= True)\n","\n","#initializing dataloaders\n","batch_size= 64\n","train_loader_fine= DataLoader(train_dataset_fine, batch_size= batch_size, shuffle= True, num_workers= 0)\n","val_loader_fine= DataLoader(val_dataset_fine, batch_size= batch_size, shuffle= False, num_workers= 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#loading pre-trained EfficientNetV2 model\n","model_pre= timm.create_model(\"hf_hub:timm/tf_efficientnetv2_b0.in1k\", pretrained= True)\n","model_pre.reset_classifier(num_classes= 2)\n","model_pre= model_pre.to('cuda')\n","print(\"Model Successfully Loaded\")\n","\n","#defining loss function, optimizer and scheduler\n","criterion= nn.CrossEntropyLoss()\n","learning_rate= 1e-4\n","optimizer= optim.AdamW(model_pre.parameters(), lr= learning_rate , betas= (0.925, 0.995), weight_decay= 0.01125, amsgrad= True)\n","scheduler= optim.lr_scheduler.StepLR(optimizer, step_size= 7, gamma= 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#fine-tuning main\n","train_model_fine(model_pre, train_loader_fine, val_loader_fine, criterion, optimizer, scheduler, num_epochs= 15)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#saving fine-truned EfficientNetV2 model\n","model_save_path = r'MODEL_SAVE_PATH'\n","torch.save(model_pre, model_save_path)\n","print(f'Model Saved to {model_save_path}')"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["#diffusion layer replacing classifier\n","class DiffusionLayer(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_classes, num_steps):\n","        super(DiffusionLayer, self).__init__()\n","        self.num_steps= num_steps\n","        \n","        self.encoder= nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim// 2),\n","            nn.ReLU()\n","        )\n","        \n","        self.decoder= nn.Sequential(\n","            nn.Linear(hidden_dim// 2, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, input_dim)\n","        )\n","        self.classifier= nn.Sequential(\n","            nn.Linear(input_dim, input_dim// 2),\n","            nn.BatchNorm1d(input_dim// 2),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(input_dim// 2, input_dim// 4),\n","            nn.BatchNorm1d(input_dim// 4),\n","            nn.Tanh(),\n","            nn.Dropout(0.6),\n","            nn.Linear(input_dim// 4, input_dim// 6),\n","            nn.BatchNorm1d(input_dim// 6),\n","            nn.SELU(),\n","            nn.Dropout(0.7),\n","            nn.Linear(input_dim// 6, num_classes)\n","        )\n","    \n","    def forward(self, x):\n","        noise_levels= [1/ (step+ 1)** 2 for step in range(self.num_steps)]\n","        for step in reversed(range(self.num_steps)):\n","            noise= torch.randn_like(x)* noise_levels[step]\n","            x_noisy= x+ noise\n","            prior= self.encoder(x)\n","            denoised= self.decoder(prior)\n","            x= x_noisy- denoised\n","        x= self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#dataset configurations\n","data_cfg= timm.data.resolve_data_config({'input_size': (3, 224, 224)})\n","data_transforms= {\n","    'train': transforms.Compose([timm.data.create_transform(**data_cfg, is_training= True),\n","                                 transforms.Normalize(mean= [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225])]),\n","    'val': transforms.Compose([timm.data.create_transform(**data_cfg, is_training= False),\n","                                 transforms.Normalize(mean= [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225])]),\n","}\n","\n","#dataset paths\n","train_data_path= r'TRAINING_IMAGES_PATHS'\n","val_data_path= r'VALIDATION_IMAGES_PATHS'\n","\n","#initializing datasets\n","train_dataset_diff= DeepfakeDataset(root_dir= train_data_path, transform= data_transforms['train'])\n","val_dataset_diff= DeepfakeDataset(root_dir= val_data_path, transform= data_transforms['val'])\n","\n","#initializing dataloaders\n","batch_size= 128\n","train_loader_diff= DataLoader(train_dataset_diff, batch_size= batch_size, shuffle= True, num_workers= 0)\n","val_loader_diff= DataLoader(val_dataset_diff, batch_size= batch_size, shuffle= False, num_workers= 0)\n","\n","#load the fine-tuned EfficientNetV2 model\n","model_fine= torch.load(r'MODEL_LOAD_PATH')\n","model_fine.train()\n","\n","#freezing layers\n","for param in model_fine.parameters():\n","    param.requires_grad= False\n","\n","#replacing classifier\n","input_dim= model_fine.classifier.in_features\n","hidden_dim= input_dim* 4\n","num_classes= 2\n","num_steps= 3\n","model_fine.classifier= DiffusionLayer(input_dim, hidden_dim, num_classes, num_steps).to('cuda')\n","\n","#unfreezing DiffusionLayer()\n","for name, param in model_fine.named_parameters():\n","  if 'classifier' in name or 'fc' in name:\n","    param.requires_grad= True\n","\n","model_fine= model_fine.to('cuda')\n","print(\"Model Successfully Loaded\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#defining loss function, optimizer and scheduler\n","learning_rate= 0.00095\n","criterion= nn.CrossEntropyLoss()\n","optimizer= optim.AdamW(filter(lambda p: p.requires_grad, model_fine.parameters()), lr= learning_rate, betas= (0.895, 0.995), weight_decay= 0.01250, amsgrad=True)\n","scheduler= optim.lr_scheduler.StepLR(optimizer, step_size= 12, gamma= 0.1111)\n","\n","#training sequence for DiffusionLayer()\n","train_model_fine(model_fine, train_loader_diff, val_loader_diff, criterion, optimizer, scheduler, num_epochs= 10, type= 'diffusion')\n","\n","#unfreezing all layers\n","for param in model_fine.parameters():\n","    param.requires_grad= True\n","learning_rate= 0.000127\n","\n","#training sequence for diffusion classification model\n","train_model_fine(model_fine, train_loader_diff, val_loader_diff, criterion, optimizer, scheduler, num_epochs= 15, type= 'diffusion')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#saving diffusion classification model\n","model_save_path = r'MODEL_SAVE_PATH'\n","torch.save(model_fine.state_dict(), model_save_path)\n","print(f'Model Saved to {model_save_path}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#load the EfficientNetV2-Diffusion model\n","model_diff= model_fine\n","model_load_state= r'MODEL_LOAD_PATH'\n","model_diff.load_state_dict(torch.load(model_load_state))\n","model_diff= model_diff.to('cuda')  # Move the model to GPU\n","model_diff.eval()\n","print(\"Model Successfully Loaded\")\n","\n","#load ground truth CSV\n","ground_truth_path= r'GROUND_TRUTH_CSV'\n","ground_truth_df= pd.read_csv(ground_truth_path)\n","\n","#dataset configurations\n","data_cfg= timm.data.resolve_data_config({'input_size': (3, 224, 224)})\n","transform= transforms.Compose([timm.data.create_transform(**data_cfg, is_training= False),\n","                                transforms.Resize((224, 224)),\n","                                 transforms.Normalize(mean= [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225])])\n","\n","#test data path\n","test_data_path= r'TEST_IMAGES_PATH'\n","\n","#loading images for testing\n","def load_image(image_path):\n","    image= Image.open(image_path).convert('RGB')\n","    image= transform(image)\n","    return image.unsqueeze(0)\n","\n","#predictions on test set\n","predictions = []\n","\n","for index, row in ground_truth_df.iterrows():\n","    start_time= time.time()\n","    image_path= os.path.join(test_data_path, row['Image'])\n","    image= load_image(image_path)\n","    image= image.to('cuda')  # Move the image to GPU\n","    with torch.no_grad():\n","        output= model_fine(image)\n","        _, predicted= torch.max(output, 1)\n","        predictions.append(predicted.item())\n","    if index% 1000== 0:\n","        print(f'Processed {index} Images.')\n","        print(\"--- %s Seconds ---\\n\" % (time.time()- start_time))\n","ground_truth_df['Prediction']= predictions\n","\n","\n","#calculating accuracy\n","accuracy= (ground_truth_df['Label']== ground_truth_df['Prediction']).mean()\n","print(f'Accuracy: {accuracy:.2f}')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
